# Enhanced Quantum Neural Network Project

A practical, VS Codeâ€“friendly playground for **Quantum Machine Learning (QML)** on the Kaggle **Cardiovascular Disease** dataset.  
You can compare a compact **classical neural net**, several **pure QML circuits**, a concise **EnhancedQNN (1Q+3C)**, and a deeper **DeepHybridQNN (3Q+3C)**â€”all on the **same preprocessing** for a fair comparison.

> All artifacts (plots, CSVs) are saved to `./outputs`. Scripts use non-interactive Matplotlib backends.

---

## ğŸ¯ Objectives
- Provide **clean baselines** (classical + multiple QML templates).
- Demonstrate **hybrid quantumâ€“classical** models where a quantum frontâ€‘end feeds a classical head.
- Keep experiments **reproducible** (fixed seeds) and **portable** (PyTorch + PennyLane).

---

## ğŸ“¦ Repository Layout
```
â”œâ”€â”€ QML_model_enhanced.py       # Baselines + EnhancedQNN runner
â”œâ”€â”€ QML_model_deep_hybrid.py    # DeepHybridQNN vs EnhancedQNN vs Classical
â”œâ”€â”€ cardio_train.csv            # Sample CSV (Kaggle cardio subset)
â”œâ”€â”€ outputs/                    # Confusion matrices (.png) & summaries (.csv)
â””â”€â”€ README.md                   # This file
```

---

## ğŸ§  Dataset & Preprocessing
**Dataset**: Kaggle *Cardiovascular Disease* training CSV (`cardio_train.csv`).

**Features (11)**: `age, gender, height, weight, ap_hi, ap_lo, cholesterol, gluc, smoke, alco, active`  
**Target (1)**: `cardio` (0/1)

**Preprocessing (shared by all experiments):**
- **Delimiter auto-detect**: try comma; if the CSV collapses to one column, retry with `sep=";"`.
- **Sanity fixes**: `age` from *days â†’ years*; clamp blood pressure to `ap_hiâˆˆ[60,240]`, `ap_loâˆˆ[30,150]`.
- **Split & scale**: stratified 80/20 split; `MinMaxScaler` on features.

**Quantum input builders (choose one):**
- **MI topâ€‘k (default)**: select the top `k = n_qubits` features using mutual information (supervised, simple).
- **PCA â†’ AmplitudeEmbedding (`--amp_embed`)**: project to `2^n_qubits` principal components and feed as a normalized amplitude vector.

---

## ğŸ§© Models â€” What They Are & How They Work

### 1) Classical Baseline (small MLP)
**Why**: establish a reference using *all 11 features*.  
**Architecture**:
```
Input(11) â†’ Linear(11â†’H) â†’ ReLU â†’ Linear(Hâ†’2) â†’ Softmax
```
- `H = max(6, in_dim)` (scales with feature count).
- **Loss**: CrossEntropyLoss (multi-class over {0,1}).
- **Optimizer**: Adagrad (simple & stable for small nets).

---

### 2) Pure QML Templates (4â€‘qubit heads)
Each QML model encodes a lengthâ€‘`n_qubits` vector `x` into a quantum state, applies a parametrized circuit, and **measures Pauliâ€‘Z expectations** on each wire:
\
`f_Î¸(x) = [âŸ¨Zâ‚€âŸ©, âŸ¨Zâ‚âŸ©, â€¦, âŸ¨Z_{n_q-1}âŸ©]`  
The result is then mapped to 2 logits via a linear layer + Softmax.

Common readout head:
```
QuantumExpectations(n_qubits) â†’ Linear(n_qubitsâ†’2) â†’ Softmax
```

**Templates included:**
- **BasicEntanglerLayers**  
  - *Encoding*: `AngleEmbedding(x)` (rotations per qubit).  
  - *Ansatz*: layers of singleâ€‘qubit rotations + ring entanglers.  
  - *Use case*: fast baseline with modest expressive power.
- **StronglyEntanglingLayers**  
  - Deeper circuit with stronger entanglement patterns.  
  - *Effect*: higher capacity at the cost of train stability; depth set via `weights` repetitions.
- **RandomLayers**  
  - Pseudorandom parametrized layers (seeded).  
  - *Use case*: stressâ€‘test whether structured entanglement matters for your data.
- **AmplitudeEmbedding**  
  - *Encoding*: normalized amplitude vector (dimension `2^n_qubits`).  
  - *When `--amp_embed` is enabled*: inputs come from PCA to match amplitude dimension.  
  - *Effect*: global, compact encoding that can capture more variance at small qubit counts.

**Training**: like the classical baseline (CrossEntropyLoss), but the forward pass is differentiable through the QNode (PennyLane `TorchLayer`).

---

### 3) EnhancedQNN (1Q + 3C) â€” *compact hybrid*
**Idea**: use a **single quantum block** to extract a non-linear representation, then process with a small classical head for binary logits.

**Block diagram**:
```
x (k dims)
 â””â”€â†’ [QBlock: Angle/Amplitude + StronglyEntanglingLayers] â†’ z âˆˆ â„^{n_qubits}
      â†’ ReLU â†’ Linear(nqâ†’8) â†’ ReLU â†’ Linear(8â†’4) â†’ Linear(4â†’1) (logit)
```
- **Embedding**: default `AngleEmbedding` on MI topâ€‘k features; optional `AmplitudeEmbedding` with PCA.  
- **Loss**: `BCEWithLogitsLoss` (numerically stable).  
- **Why it works**: the quantum layer can reshape feature interactions; the classical head aggregates them into a decision boundary with low parameter count.

---

### 4) DeepHybridQNN (3Q + 3C) â€” *stacked hybrid with stabilizers*
**Idea**: alternate **three** quantum blocks with classical transforms, allowing information to be iteratively reâ€‘encoded and reâ€‘entangled.

**Macro architecture**:
```
x â†’ Q1 â†’ C1 â†’ (angle_squash) â†’ Q2 â†’ C2 â†’ Dropout â†’ (angle_squash) â†’ Q3 â†’ C3(â†’4) â†’ Out(â†’1 logit)
```
- **Q blocks**: each is `Angle/AmplitudeEmbedding + StronglyEntanglingLayers` (depth via `--q_reps_block`).  
- **C blocks**: linear transforms matching `n_qubits` width (`C1`, `C2`) + a `C3` projection to 4 dims.  
- **Stabilizers**:
  - `angle_squash`: `x â† tanh(x) * cap`, with `cap âˆˆ {Ï€, Ï€/2}` (use `--half_angles` for Â±Ï€/2) to keep angles in a reasonable range and reduce saturation.
  - Optional **LayerNorm** after `C1`/`C2` (`--use_layernorm`) to stabilize distributional shifts.
  - **Dropout** after `C2` (`--dropout_p`) to regularize.  
- **Loss**: `BCEWithLogitsLoss`.

**Why stack?**  
Alternating Q/C blocks lets the model *re-embed* intermediate features into the quantum state space, potentially capturing higherâ€‘order interactions that a single Q block might miss.

---

## âš™ï¸ Setup

### 1) Clone & enter
```bash
git clone https://github.com/Vatsal212005/Enhanced-Quantum-Neural-Network-Project.git
cd Enhanced-Quantum-Neural-Network-Project
```

### 2) Create & activate a virtual environment
```bash
python -m venv venv
# Activate:
# Linux/Mac
source venv/bin/activate
# Windows (PowerShell)
venv\Scripts\Activate.ps1
```

### 3) Install dependencies
If you already have a `requirements.txt`, use it. Otherwise:
```bash
pip install numpy pandas torch scikit-learn matplotlib seaborn pennylane
```

---

## â–¶ï¸ How to Run

### A) Baselines + EnhancedQNN
Runs classical + multiple QML templates + EnhancedQNN end-to-end.
```bash
python QML_model_enhanced.py \
  --csv ./cardio_train.csv \
  --epochs 20 \
  --enhanced_epochs 15 \
  --lr 5e-3 \
  --enhanced_lr 1e-2 \
  --outdir ./outputs
```
**Outputs**
- `outputs/cm_classical.png`, `cm_qml_basic.png`, `cm_qml_strong.png`, `cm_qml_random.png`, `cm_qml_amp.png`, `cm_enhanced_qnn.png`
- `outputs/summary.csv` (model vs accuracy)

---

### B) DeepHybridQNN vs EnhancedQNN vs Classical
Compares the deeper hybrid architecture with tunable knobs.
```bash
python QML_model_deep_hybrid.py \
  --csv ./cardio_train.csv \
  --model all \
  --epochs_classical 20 \
  --epochs_enhanced 25 \
  --epochs_deep 25 \
  --lr_classical 5e-3 \
  --lr_enhanced 5e-3 \
  --lr_deep 1e-3 \
  --n_qubits 4 \
  --q_reps_enh 8 \
  --q_reps_block 6 \
  --batch_size 256 \
  --outdir ./outputs
```
**Useful flags**
- `--model {all,classical,enhanced,deep}`: run a subset.
- `--half_angles`: use Â±Ï€/2 caps before Q2/Q3 (`angle_squash`) to reduce saturation.
- `--dropout_p 0.15`: dropout after C2.
- `--use_layernorm`: LayerNorm after C1/C2.
- `--amp_embed`: switch to PCA + `AmplitudeEmbedding` instead of MI + `AngleEmbedding`.

**Outputs**
- `outputs/cm_classical.png`, `cm_enhanced_qnn.png`, `cm_deep_hybrid_qnn.png` (depending on `--model`)
- `outputs/summary_deep_vs_enhanced.csv`

---

## ğŸ”¬ Training & Evaluation Details
- **Optimizers**: `Adagrad` for CE baselines; `Adam` for hybrid BCE models.
- **Losses**:  
  - CE models (classical & QML templates): `CrossEntropyLoss` (2â€‘class).  
  - Hybrid models (Enhanced/Deep): `BCEWithLogitsLoss` with sigmoid at eval time.
- **Batching**: DataLoader with `batch_size` (default 256 for hybrids).
- **Metrics**: Accuracy, confusion matrix (PNG), full `classification_report` (printed).
- **Reproducibility**: `set_seed(42)` (+ deterministic cuDNN flags).

---

## ğŸ“ˆ Artifacts
All saved to `./outputs`:
- `cm_*.png` â€” confusion matrices for each model variant.
- `summary.csv` â€” accuracy leaderboard for baselines + EnhancedQNN.
- `summary_deep_vs_enhanced.csv` â€” accuracy comparison for classical vs EnhancedQNN vs DeepHybridQNN.

---

## ğŸ§­ Extensions / Ideas
- Try different `n_qubits` (e.g., 6 or 8) and compare capacity vs overfitting.
- Explore learning-rate schedules, weight decay, or gradient clipping in hybrids.
- Swap `StronglyEntanglingLayers` with custom ansÃ¤tze or hardware-efficient patterns.
- Evaluate calibration (e.g., reliability diagrams) for the logits from hybrid heads.
- Add ROCâ€‘AUC/PRâ€‘AUC for classâ€‘imbalance sensitivity.

---

## ğŸ“œ License
MIT â€” free to use and modify with attribution.
